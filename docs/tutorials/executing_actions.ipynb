{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Executing Actions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial 1: Simple Actions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the heart of **ralf** is the ``Action`` class. An ``Action`` object represents\n",
    "a basic data processing block and can take one of two forms: (1) a call to a \n",
    "Large Language Model (LLM) or (2) a Python function.\n",
    "\n",
    "The first example we will look at is defining and executing simple LLM-based action.\n",
    "We begin by importing the ```Action``` and defining our action. When defining\n",
    "an action, we can provide a prompt that we expect that LLM to complete for us. In this\n",
    "example we are looking for the LLM to tell us what the capital of Australia is.\n",
    "After defining the action, we can use the object's ```__call__``` method to execute it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output': 'Canberra.'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ralf.dispatcher import Action\n",
    "\n",
    "get_aussie_capital = Action(prompt=\"The capital of Australia is\")\n",
    "get_aussie_capital()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in the above example, we did not specify anything about the LLM itself. \n",
    "In this case, **ralf** used the default LLM configuration, which is specified in\n",
    "``ralf.utils``. However, we can also directly specify one or more model configuration\n",
    "parameters when creating the action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output': 'Canberra'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_aussie_capital = Action(\n",
    "    prompt=\"The capital of Australia is\",\n",
    "    model_config={\n",
    "        'model': 'gpt-3.5-turbo',\n",
    "        'temperature': 0.0,\n",
    "        'stop': ['.']\n",
    "    }\n",
    ")\n",
    "get_aussie_capital()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial 2: Actions with Context\n",
    "\n",
    "So far, we have seen how to create an LLM-based action with a full prompt,\n",
    "in which case we simply want to obtain a completion of a fully-formed piece of text.\n",
    "In many situations, we do not have a full prompt, but instead have a *prompt template*.\n",
    "A prompt template contains placeholders that need to be filled in with appropriate context\n",
    "before being submitted to the LLM for completion\n",
    "\n",
    "For example, we may want to define an action that can ask an LLM for the capital of\n",
    "any country. Then, when we go to execute the action, we can provide the name of\n",
    "the country whose capital we wish to know. We can do this by providing a prompt\n",
    "template when defining the action, then providing a context dictionary with entries\n",
    "corresponding to the placeholders when we execute the action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output': 'Cairo.'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_country_capital = Action(prompt_template=\"The capital of {country} is\")\n",
    "get_country_capital(context={'country': 'Egypt'})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial 3: Python-Based Actions\n",
    "\n",
    "Remember that **ralf** actions can either involve calls to LLMs or execution of \n",
    "Python functions. In this example, we will explore the latter. To define a Python-based\n",
    "action, you can simply pass in a Python function when creating the action object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'summed_numbers': 33}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def sum_from_text(text: str) -> int:\n",
    "    \"\"\"Finds all numbers in the text and sums them\"\"\"\n",
    "    \n",
    "    text_counts = re.findall('[0-9]+', text)\n",
    "    return sum([int(x) for x in text_counts])\n",
    "\n",
    "count_adder = Action(func=sum_from_text,\n",
    "                     input_name='string_with_numbers',\n",
    "                     output_name='summed_numbers')\n",
    "\n",
    "count_adder({\"string_with_numbers\" : \"12, 5, 10 and 6\"})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While defining an action to simply execute a Python function might seem trivial at first,\n",
    "we will see how this is useful when creating a sequence of actions in the next example."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial 3: Action Sequences"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In many applications, we want to execute a sequence of steps that processes some information\n",
    "and arrives at a final result. Some steps in this process might be very well defined (e.g., arithmetic)\n",
    "or might involve some interaction with other external resources (e.g., querying a database), in which\n",
    "case you might write Python functions to implement them. In other cases, you might wish to\n",
    "use a call to an LLM to exectue the task. **ralf** makes it easy to chain together actions of \n",
    "both types, as we will see in the next example.\n",
    "\n",
    "Say we have a piece of text that represents a customer's order. We'd like to determine how many fruits are in the customer's order and generate a natural language response to them based on the number of fruits. We can begin by creating the actions invovled in the process. Here, we will have three actions. The first will use an LLM's knowledge of fruits, and its language understanding capabilities, to interpret a user's order and enumerate the number of fruits of each type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "enumerate_fruits = Action(\n",
    "                    prompt_template=\"I'm going to give you sentence. \"\n",
    "                    \"Please enumerate how many fruits of each type are mentioned. Ignore non-fruits. \"\n",
    "                    \"Format should be fruit_name:<fruit_count> with commas in between. \"\n",
    "                    \"Sentence: {utterance}\",\n",
    "                    output_name='fruit_counts'\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next action will take the output of the previous fruit enumeration action and sum the counts together to arrive at a total number of fruits. The reason we may want to do this is to avoid relying on the LLMs arthmetic capabilities, which have been shown to be unreliable (though progress is being made on this front). Since we know how to do basic arthmetic in a reliable manner, it is more appropriate to use a Python function for this step. We can create this action using the ``sum_from_text`` function we defined previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_fruits = Action(func=sum_from_text,\n",
    "                    input_name='fruit_counts',\n",
    "                    output_name='fruit_total'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we create a third action that will draft a response to the customer based on the fruit total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_reply = Action(\n",
    "    prompt_template=\"A customer is trying to purchase {fruit_total} fruits from our company Fruits'R'Us. \"\n",
    "                    \"Write a reply to them restating their order and explaining the policy if they exceed \"\n",
    "                    \"the maximum of 10 fruits per order. Otherwise politely thank them for their business.\",\n",
    "    output_name='reply'\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will see how we can execute all 3 actions in a sequence, with appropriate\n",
    "passing of the outputs of one action into the inputs of the next. To do this, we \n",
    "will leverage the ``Dispatcher`` class within **ralf**. The job of the dispatcher\n",
    "object is to handle the details of how exactly to execute an action, or a sequence of actions.\n",
    "\n",
    "In this example, we will find out how to execute an action sequence. We begin by \n",
    "defining a dispatcher object. Then, we simply string together the three actions \n",
    "we just defined by wrapping them into a standard Python list. Now, we call the \n",
    "``execute`` method in the dispatcher object-- passing it the list of actions and the customer's order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dear valued customer,\n",
      "\n",
      "Thank you for choosing Fruits'R'Us for your fruit needs. We appreciate your business.\n",
      "\n",
      "I understand that you would like to purchase 12 fruits from us. To confirm, you are requesting 12 fruits in total, is that correct?\n",
      "\n",
      "Please note that our policy allows a maximum of 10 fruits per order. However, we would be happy to assist you in placing multiple orders if you require more than 10 fruits.\n",
      "\n",
      "Thank you for your understanding and please let us know how we can assist you further.\n",
      "\n",
      "Best regards,\n",
      "\n",
      "[Your name]\n"
     ]
    }
   ],
   "source": [
    "from ralf.dispatcher import ActionDispatcher\n",
    "ad = ActionDispatcher()\n",
    "\n",
    "script = [enumerate_fruits, sum_fruits, create_reply]\n",
    "\n",
    "input_text = \"I'd like to order 4 bananas, 6 oranges, a cabbage and 2 honeycrips\"\n",
    "output, _ = ad.execute(script, utterance=input_text)\n",
    "\n",
    "print(output['reply'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial 4: Using YAML Files"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, the LLM prompts we have seen have been relatively simple and short. In\n",
    "many applications, however, prompts (or prompt templates) can contain large amounts\n",
    "of text. In such cases, it is inconvenient to store the prompts (or prompt templates)\n",
    "inside the source code, and it is recommended that one use YAML files to store\n",
    "these instead. \n",
    "\n",
    "With **ralf**, we can easily perform YAML-based prompt template specification when defining \n",
    "an object from the ```Dispatcher``` class. The ```Dispatcher``` constructor takes\n",
    "as input the path to what we call a *RALF data directory* (typically named ```ralf_data```).\n",
    "Inside of this directory is where we place the YAML file that contains the prompt templates\n",
    "we want our dispatcher to know about (the file should be named ```prompts.yml```). An example\n",
    "```prompts.yml``` file can be found in the ```demos``` directory. \n",
    "\n",
    "In addition to ```prompts.yml```, the RALF data directory should also contain a second \n",
    "YAML file for specifying model configurations. For each prompt template in ```prompts.yml```\n",
    "with an associated model name specified, **ralf** will excite LLM-based actions using the\n",
    "model configuration of this name in ```models.yml```.\n",
    "\n",
    "We can see in the example code below that using YAML-based prompt template and model\n",
    "specification can greatly simplify our Python code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'output': \" 'strawberries':2, 'bananas':4\"}\n"
     ]
    }
   ],
   "source": [
    "from ralf.dispatcher import ActionDispatcher\n",
    "\n",
    "ad = ActionDispatcher(dir='../../demos/ralf_data')\n",
    "\n",
    "enumerate_fruits = Action(prompt_name='enumerate_fruits')\n",
    "output, _ = ad.execute([enumerate_fruits], utterance='i have 2 strawberry shortcakes, 4 bananas and 6 hot dogs')\n",
    "\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "24f65b38c0964cb28db85bec3ca6c1dc96944e5978a2622bfc3ff8c4d709aefe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
